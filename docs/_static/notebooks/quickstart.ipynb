{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LittleMCMC Quickstart\n",
    "\n",
    "LittleMCMC is a lightweight and performant implementation of HMC and NUTS in Python, spun out of the PyMC project. In this quickstart tutorial, we will walk through the main use case of LittleMCMC, and outline the various modules that may be of interest.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Who should use LittleMCMC?](#who-should-use-littlemcmc)\n",
    "- [Sampling](#how-to-sample)\n",
    "  - [Inspecting the Output of lmc.sample](#inspecting-the-output-of-lmc-sample)\n",
    "- [Customizing the Default NUTS Sampler](#customizing-the-default-nuts-sampler)\n",
    "- [Other Modules](#other-modules)\n",
    "\n",
    "## Who should use LittleMCMC?\n",
    "\n",
    "LittleMCMC is a fairly barebones library with a very niche use case. Most users will probably find that [PyMC3](https://github.com/pymc-devs/pymc3) will satisfy their needs, with better strength of support and quality of documentation.\n",
    "\n",
    "There are two expected use cases for LittleMCMC. Firstly, if you:\n",
    "\n",
    "1. Have a model with only continuous parameters,\n",
    "1. Are willing to manually transform all of your model's parameters to the unconstrained space (if necessary),\n",
    "1. Have a Python function/callable that:\n",
    "   1. computes the log probability of your model and its derivative\n",
    "   1. is [pickleable](https://docs.python.org/3/library/pickle.html)\n",
    "   1. outputs an array with the same shape as its input\n",
    "1. And all you need is an implementation of HMC/NUTS (preferably in Python) to sample from the posterior,\n",
    "\n",
    "then you should consider using LittleMCMC.\n",
    "\n",
    "Secondly, if you want to run algorithmic experiments on HMC/NUTS (in Python), without having to develop around the heavy machinery that accompanies other probabilistic programming frameworks (like [PyMC3](https://github.com/pymc-devs/pymc3/), [TensorFlow Probability](https://github.com/tensorflow/probability/) or [Stan](https://github.com/stan-dev/stan)), then you should consider running your experiments in LittleMCMC.\n",
    "\n",
    "## How to Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import littlemcmc as lmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logp_func(x, loc=0, scale=1):\n",
    "    return np.log(scipy.stats.norm.pdf(x, loc=loc, scale=scale))\n",
    "\n",
    "\n",
    "def dlogp_func(x, loc=0, scale=1):\n",
    "    return -(x - loc) / scale\n",
    "\n",
    "\n",
    "def logp_dlogp_func(x, loc=0, scale=1):\n",
    "    return logp_func(x, loc=loc, scale=scale), dlogp_func(x, loc=loc, scale=scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/george/littlemcmc/venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# By default: 4 chains in 4 cores, 500 tuning steps and 1000 sampling steps.\n",
    "trace, stats = lmc.sample(\n",
    "    logp_dlogp_func=logp_dlogp_func,\n",
    "    model_ndim=1,\n",
    "    progressbar=None,  # HTML progress bars don't render well in RST.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Output of `lmc.sample`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1000, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape is (num_chains, num_samples, num_parameters)\n",
    "trace.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.92958231],\n",
       "        [ 0.92958231]],\n",
       "\n",
       "       [[-1.06231693],\n",
       "        [-1.11589309]],\n",
       "\n",
       "       [[-0.73177109],\n",
       "        [-0.66975061]],\n",
       "\n",
       "       [[ 0.8923907 ],\n",
       "        [ 0.97253646]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first 2 samples across all chains and parameters\n",
    "trace[:, :2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['depth', 'step_size', 'tune', 'mean_tree_accept', 'step_size_bar', 'tree_size', 'diverging', 'energy_error', 'energy', 'max_energy_error', 'model_logp'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1000, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again, shape is (num_chains, num_samples, num_parameters)\n",
    "stats[\"depth\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2],\n",
       "        [1]],\n",
       "\n",
       "       [[1],\n",
       "        [1]],\n",
       "\n",
       "       [[2],\n",
       "        [1]],\n",
       "\n",
       "       [[2],\n",
       "        [1]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first 2 tree depths across all chains and parameters\n",
    "stats[\"depth\"][:, :2, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing the Default NUTS Sampler\n",
    "\n",
    "By default, `lmc.sample` samples using NUTS with sane defaults. These defaults can be override by either:\n",
    "\n",
    "1. Passing keyword arguments from `lmc.NUTS` into `lmc.sample`, or\n",
    "2. Constructing an `lmc.NUTS` sampler, and passing that to `lmc.sample`. This method also allows you to choose to other samplers, such as `lmc.HamiltonianMC`.\n",
    "\n",
    "For example, suppose you want to increase the `target_accept` from the default `0.8` to `0.9`. The following two cells are equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace, stats = lmc.sample(\n",
    "    logp_dlogp_func=logp_dlogp_func,\n",
    "    model_ndim=1,\n",
    "    target_accept=0.9,\n",
    "    progressbar=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/george/littlemcmc/venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "step = lmc.NUTS(logp_dlogp_func=logp_dlogp_func, model_ndim=1, target_accept=0.9)\n",
    "trace, stats = lmc.sample(\n",
    "    logp_dlogp_func=logp_dlogp_func,\n",
    "    model_ndim=1,\n",
    "    step=step,\n",
    "    progressbar=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a list of keyword arguments that `lmc.NUTS` accepts, please refer to the [API reference for `lmc.NUTS`](https://littlemcmc.readthedocs.io/en/latest/generated/littlemcmc.NUTS.html#littlemcmc.NUTS).\n",
    "\n",
    "## Other Modules\n",
    "\n",
    "LittleMCMC exposes:\n",
    "\n",
    "1. Two step methods (a.k.a. samplers): [`littlemcmc.HamiltonianMC` (Hamiltonian Monte Carlo)](https://littlemcmc.readthedocs.io/en/latest/generated/littlemcmc.HamiltonianMC.html#littlemcmc.HamiltonianMC) and the [`littlemcmc.NUTS` (No-U-Turn Sampler)](https://littlemcmc.readthedocs.io/en/latest/generated/littlemcmc.NUTS.html#littlemcmc.NUTS)\n",
    "1. Various quadpotentials (a.k.a. mass matrices or inverse metrics) in [`littlemcmc.quadpotential`](https://littlemcmc.readthedocs.io/en/latest/api.html#quadpotentials-a-k-a-mass-matrices), along with mass matrix adaptation routines\n",
    "1. Dual-averaging step size adaptation in [`littlemcmc.step_sizes`](https://littlemcmc.readthedocs.io/en/latest/generated/littlemcmc.step_sizes.DualAverageAdaptation.html#littlemcmc.step_sizes.DualAverageAdaptation)\n",
    "1. A leapfrog integrator in [`littlemcmc.integration`](https://littlemcmc.readthedocs.io/en/latest/generated/littlemcmc.integration.CpuLeapfrogIntegrator.html#littlemcmc.integration.CpuLeapfrogIntegrator)\n",
    "\n",
    "These modules should allow for easy experimentation with the sampler. Please refer to the [API Reference](https://littlemcmc.readthedocs.io/en/latest/api.html) for more information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
